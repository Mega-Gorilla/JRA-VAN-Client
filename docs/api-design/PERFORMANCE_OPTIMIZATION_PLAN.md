# REST API ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–è¨ˆç”»

## 1. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ

### 1.1 ç¾çŠ¶ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ§‹æˆ
```
ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¦æ±‚
    â”œâ”€ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¾€å¾©: 1-5ms
    â”œâ”€ FastAPIå‡¦ç†: 1-3ms
    â”œâ”€ JVLink COMå‘¼ã³å‡ºã—: 10-30ms
    â”œâ”€ ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ã‚¹: 5-15ms
    â”œâ”€ JSONå¤‰æ›: 2-5ms
    â””â”€ ãƒ¬ã‚¹ãƒãƒ³ã‚¹è¿”é€: 1-5ms
    
åˆè¨ˆ: 20-63msï¼ˆå¹³å‡ç´„40msï¼‰
```

### 1.2 ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
| ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ | å½±éŸ¿åº¦ | æ”¹å–„å¯èƒ½æ€§ | å„ªå…ˆåº¦ |
|------------|--------|-----------|--------|
| JVLink COMå‘¼ã³å‡ºã— | é«˜ï¼ˆ50%ï¼‰ | ä¸­ | é«˜ |
| ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ã‚¹ | ä¸­ï¼ˆ25%ï¼‰ | é«˜ | é«˜ |
| å¤§é‡ãƒ‡ãƒ¼ã‚¿è»¢é€ | é«˜ï¼ˆçŠ¶æ³ä¾å­˜ï¼‰ | é«˜ | é«˜ |
| åŒæœŸçš„å‡¦ç† | ä¸­ï¼ˆ20%ï¼‰ | é«˜ | ä¸­ |

## 2. ã‚·ãƒ³ãƒ—ãƒ«ãª2å±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥

### 2.1 ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          L1: ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥                â”‚
â”‚          (TTLCache, 0.01ms)                 â”‚
â”‚          å®¹é‡: 200-500ã‚¨ãƒ³ãƒˆãƒª               â”‚
â”‚          TTL: 1-5åˆ†                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹æ™‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          L2: Redis (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)              â”‚
â”‚          (åˆ†æ•£ã‚­ãƒ£ãƒƒã‚·ãƒ¥, 1-2ms)            â”‚
â”‚          å®¹é‡: ç„¡åˆ¶é™                        â”‚
â”‚          TTL: ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥ä¾å­˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹æ™‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          JVLink COM                          â”‚
â”‚          (ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ãƒã‚¤ãƒ³ãƒˆ**: SQLiteã¯ä¸è¦ã€‚JVLinkè‡ªä½“ãŒãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ãŸã‚ã€è¿½åŠ ã®æ°¸ç¶šåŒ–å±¤ã¯è¤‡é›‘æ€§ã‚’å¢—ã™ã ã‘ã§ã™ã€‚

### 2.2 ã‚·ãƒ³ãƒ—ãƒ«ã§åŠ¹æœçš„ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Ÿè£…

```python
# simple_cache_service.py
from typing import Optional, Any, Dict
import json
import hashlib
import redis
from cachetools import TTLCache
import pickle
import asyncio

class SimpleTwoLayerCache:
    """ã‚·ãƒ³ãƒ—ãƒ«ã§é«˜æ€§èƒ½ãª2å±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        # L1: è¶…é«˜é€Ÿãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.l1_cache = TTLCache(maxsize=500, ttl=300)  # 5åˆ†
        
        # L2: Redis (ã‚ªãƒ—ã‚·ãƒ§ãƒ³ - ç„¡ãã¦ã‚‚å‹•ä½œ)
        try:
            self.redis = redis.Redis.from_url(redis_url, decode_responses=False)
            self.redis.ping()  # æ¥ç¶šç¢ºèª
            self.redis_available = True
        except:
            self.redis = None
            self.redis_available = False
            print("Redis not available, using memory cache only")
    
    def _make_key(self, dataspec: str, **kwargs) -> str:
        """åŠ¹ç‡çš„ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        key_dict = {"ds": dataspec, **kwargs}
        key_str = json.dumps(key_dict, sort_keys=True)
        # MD5ãƒãƒƒã‚·ãƒ¥ã®æœ€åˆã®16æ–‡å­—ã§ååˆ†
        return hashlib.md5(key_str.encode()).hexdigest()[:16]
    
    async def get(self, dataspec: str, **kwargs) -> Optional[Any]:
        """2å±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        cache_key = self._make_key(dataspec, **kwargs)
        
        # L1: ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª (0.01ms)
        if cache_key in self.l1_cache:
            return self.l1_cache[cache_key]
        
        # L2: Redisç¢ºèª (1-2ms)
        if self.redis_available:
            try:
                data = self.redis.get(f"jra:{cache_key}")
                if data:
                    result = pickle.loads(data)
                    self.l1_cache[cache_key] = result  # L1ã«æ˜‡æ ¼
                    return result
            except Exception as e:
                print(f"Redis error: {e}")
                # Redisã‚¨ãƒ©ãƒ¼ã§ã‚‚å‡¦ç†ç¶™ç¶š
        
        return None
    
    async def set(self, data: Any, dataspec: str, **kwargs):
        """2å±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        cache_key = self._make_key(dataspec, **kwargs)
        ttl = self._get_optimal_ttl(dataspec)
        
        # L1: ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«å³åº§ã«ä¿å­˜
        self.l1_cache[cache_key] = data
        
        # L2: Redisã«éåŒæœŸä¿å­˜ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if self.redis_available:
            asyncio.create_task(self._save_to_redis(cache_key, data, ttl))
    
    def _get_optimal_ttl(self, dataspec: str) -> int:
        """ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥ã«å¿œã˜ãŸæœ€é©ãªTTLï¼ˆç§’ï¼‰"""
        ttl_map = {
            # ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆé•·æœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
            'HOSE': 86400,  # 24æ™‚é–“
            'HOYU': 86400,  # 24æ™‚é–“
            'YSCH': 604800,  # 1é€±é–“
            'RACE': 86400,  # 24æ™‚é–“ï¼ˆç¢ºå®šå¾Œä¸å¤‰ï¼‰
            
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ï¼ˆçŸ­æœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
            '0B12': 60,     # 1åˆ†ï¼ˆå˜è¤‡ã‚ªãƒƒã‚ºï¼‰
            '0B15': 60,     # 1åˆ†ï¼ˆé¦¬é€£ã‚ªãƒƒã‚ºï¼‰
            '0B30': 300,    # 5åˆ†ï¼ˆãƒ¬ãƒ¼ã‚¹çµæœé€Ÿå ±ï¼‰
            
            # ãã®ä»–
            'DIFF': 300,    # 5åˆ†
            'SLOP': 1800,   # 30åˆ†
            'MING': 3600,   # 1æ™‚é–“
        }
        return ttl_map.get(dataspec[:4], 600)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10åˆ†
    
    async def _save_to_redis(self, key: str, data: Any, ttl: int):
        """Redisã¸ã®éåŒæœŸä¿å­˜"""
        try:
            if ttl > 0:
                serialized = pickle.dumps(data)
                self.redis.setex(f"jra:{key}", ttl, serialized)
        except Exception as e:
            print(f"Redis save error: {e}")
            # ã‚¨ãƒ©ãƒ¼ã§ã‚‚å‡¦ç†ç¶™ç¶š

# ä½¿ç”¨ä¾‹: FastAPIã§ã®å®Ÿè£…
@app.get("/api/v1/data/{dataspec}")
async def get_data(dataspec: str, from_time: str = ""):
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    cached = await cache.get(dataspec, from_time=from_time)
    if cached:
        return {"data": cached, "cache": "hit"}
    
    # JVLinkã‹ã‚‰å–å¾—
    data = await fetch_from_jvlink(dataspec, from_time)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    await cache.set(data, dataspec, from_time=from_time)
    
    return {"data": data, "cache": "miss"}

# è³¢ã„ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒãƒ³ã‚°æˆ¦ç•¥
class SmartPrefetchManager:
    def __init__(self, cache: SimpleTwoLayerCache):
        self.cache = cache
        self.prefetch_queue = asyncio.Queue()
        self.prefetch_task = None
    
    async def start(self):
        """ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒãƒ¯ãƒ¼ã‚«ãƒ¼èµ·å‹•"""
        self.prefetch_task = asyncio.create_task(self._prefetch_worker())
    
    async def _prefetch_worker(self):
        """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒ"""
        while True:
            try:
                request = await self.prefetch_queue.get()
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
                cached = await self.cache.get(**request)
                if not cached:
                    # JVLinkã‹ã‚‰å–å¾—ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                    # â€»å®Ÿéš›ã®JVLinkå‘¼ã³å‡ºã—ã‚³ãƒ¼ãƒ‰
                    pass
                    
            except Exception as e:
                print(f"Prefetch error: {e}")
            
            await asyncio.sleep(0.1)
    
    async def schedule_prefetch(self, dataspec: str, **kwargs):
        """ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«"""
        # é–¢é€£ãƒ‡ãƒ¼ã‚¿ã‚’äºˆæ¸¬ã—ã¦ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒ
        if dataspec == 'RACE':
            # ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—æ™‚ã¯é–¢é€£ã‚ªãƒƒã‚ºã‚‚ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒ
            await self.prefetch_queue.put({
                'dataspec': '0B12',
                **kwargs
            })
            await self.prefetch_queue.put({
                'dataspec': '0B15',
                **kwargs
            })
```

## 3. éåŒæœŸå‡¦ç†æœ€é©åŒ–

### 3.1 æ¥ç¶šãƒ—ãƒ¼ãƒ«ç®¡ç†

```python
# connection_pool.py
import asyncio
from typing import List, Optional
import win32com.client
import pythoncom
from contextlib import asynccontextmanager
from queue import Queue
import threading

class JVLinkConnectionPool:
    """JVLinkæ¥ç¶šãƒ—ãƒ¼ãƒ«ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰"""
    
    def __init__(self, pool_size: int = 5):
        self.pool_size = pool_size
        self.connections = Queue(maxsize=pool_size)
        self.semaphore = asyncio.Semaphore(pool_size)
        self._initialize_pool()
    
    def _initialize_pool(self):
        """æ¥ç¶šãƒ—ãƒ¼ãƒ«ã®åˆæœŸåŒ–"""
        for _ in range(self.pool_size):
            # å„æ¥ç¶šç”¨ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã§åˆæœŸåŒ–
            thread = threading.Thread(target=self._create_connection)
            thread.start()
            thread.join()
    
    def _create_connection(self):
        """COMæ¥ç¶šã®ä½œæˆï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰å†…ï¼‰"""
        pythoncom.CoInitialize()
        try:
            jvlink = win32com.client.Dispatch("JVDTLab.JVLink")
            self.connections.put(jvlink)
        finally:
            pythoncom.CoUninitialize()
    
    @asynccontextmanager
    async def acquire(self):
        """æ¥ç¶šã®å–å¾—"""
        async with self.semaphore:
            # éåŒæœŸã§ã‚»ãƒãƒ•ã‚©ã‚’å–å¾—
            connection = await asyncio.get_event_loop().run_in_executor(
                None, self.connections.get
            )
            try:
                yield connection
            finally:
                # æ¥ç¶šã‚’ãƒ—ãƒ¼ãƒ«ã«æˆ»ã™
                await asyncio.get_event_loop().run_in_executor(
                    None, self.connections.put, connection
                )

# éåŒæœŸãƒãƒƒãƒå‡¦ç†
class AsyncBatchProcessor:
    def __init__(self, pool: JVLinkConnectionPool):
        self.pool = pool
    
    async def batch_get_data(self, requests: List[Dict]) -> List[Any]:
        """è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã®ä¸¦åˆ—å–å¾—"""
        tasks = []
        for request in requests:
            task = asyncio.create_task(
                self._get_single_data(**request)
            )
            tasks.append(task)
        
        # ä¸¦åˆ—å®Ÿè¡Œ
        results = await asyncio.gather(*tasks)
        return results
    
    async def _get_single_data(self, dataspec: str, **kwargs):
        """å˜ä¸€ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆéåŒæœŸï¼‰"""
        async with self.pool.acquire() as jvlink:
            # ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã§å®Ÿè¡Œ
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,
                self._jvlink_read,
                jvlink,
                dataspec,
                kwargs
            )
            return result
    
    def _jvlink_read(self, jvlink, dataspec, kwargs):
        """JVLinkèª­ã¿è¾¼ã¿ï¼ˆåŒæœŸï¼‰"""
        # COMå‘¼ã³å‡ºã—ã¯ã‚¹ãƒ¬ãƒƒãƒ‰å†…ã§å®Ÿè¡Œ
        pythoncom.CoInitialize()
        try:
            # å®Ÿéš›ã®JVLinkå‘¼ã³å‡ºã—
            ret = jvlink.JVOpen(dataspec, kwargs.get('from_time', ''), 1, 0, 0, '')
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å‡¦ç†
            return self._read_all_data(jvlink)
        finally:
            pythoncom.CoUninitialize()
```

## 4. ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ãƒ»è»¢é€æœ€é©åŒ–

### 4.1 ãƒ¬ã‚¹ãƒãƒ³ã‚¹åœ§ç¸®

```python
# compression.py
from fastapi import Response
from fastapi.responses import StreamingResponse
import gzip
import brotli
import lz4.frame
from typing import Any, Dict
import json
import msgpack
import orjson

class CompressionMiddleware:
    """å‹•çš„åœ§ç¸®ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢"""
    
    def __init__(self, app):
        self.app = app
    
    async def __call__(self, scope, receive, send):
        if scope['type'] == 'http':
            headers = dict(scope['headers'])
            accept_encoding = headers.get(b'accept-encoding', b'').decode()
            
            # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒã‚µãƒãƒ¼ãƒˆã™ã‚‹åœ§ç¸®æ–¹å¼ã‚’ç¢ºèª
            if 'br' in accept_encoding:
                compression = 'br'
            elif 'gzip' in accept_encoding:
                compression = 'gzip'
            elif 'lz4' in accept_encoding:
                compression = 'lz4'
            else:
                compression = None
            
            scope['compression'] = compression
        
        await self.app(scope, receive, send)

# åŠ¹ç‡çš„ãªã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
class OptimizedSerializer:
    @staticmethod
    def serialize(data: Any, format: str = 'json') -> bytes:
        """é«˜é€Ÿã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³"""
        if format == 'json':
            # orjsonã¯æ¨™æº–jsonã‚ˆã‚Š3-10å€é«˜é€Ÿ
            return orjson.dumps(data)
        elif format == 'msgpack':
            # ãƒã‚¤ãƒŠãƒªå½¢å¼ã§ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆ
            return msgpack.packb(data)
        elif format == 'pickle':
            # Pythonå°‚ç”¨ã ãŒæœ€é€Ÿ
            return pickle.dumps(data, protocol=5)
    
    @staticmethod
    def compress(data: bytes, method: str = 'gzip') -> bytes:
        """ãƒ‡ãƒ¼ã‚¿åœ§ç¸®"""
        if method == 'gzip':
            return gzip.compress(data, compresslevel=6)
        elif method == 'br':
            return brotli.compress(data, quality=4)
        elif method == 'lz4':
            return lz4.frame.compress(data)
        return data

# ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹
class StreamingOptimizer:
    @staticmethod
    async def stream_large_dataset(dataspec: str, jvlink):
        """å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é…ä¿¡"""
        
        async def generate():
            buffer = []
            buffer_size = 0
            max_buffer_size = 1024 * 100  # 100KB
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼é€ä¿¡
            yield b'{"records":['
            first = True
            
            # JVLinkã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’é€æ¬¡èª­ã¿è¾¼ã¿
            while True:
                data = await read_next_record(jvlink)
                if not data:
                    break
                
                # JSONåŒ–
                json_data = orjson.dumps(data)
                
                if not first:
                    json_data = b',' + json_data
                first = False
                
                buffer.append(json_data)
                buffer_size += len(json_data)
                
                # ãƒãƒƒãƒ•ã‚¡ãŒæº€ãŸã•ã‚ŒãŸã‚‰é€ä¿¡
                if buffer_size >= max_buffer_size:
                    yield b''.join(buffer)
                    buffer = []
                    buffer_size = 0
            
            # æ®‹ã‚Šã®ãƒãƒƒãƒ•ã‚¡ã‚’é€ä¿¡
            if buffer:
                yield b''.join(buffer)
            
            yield b']}'
        
        return StreamingResponse(
            generate(),
            media_type="application/json",
            headers={
                "Content-Encoding": "gzip",
                "Transfer-Encoding": "chunked"
            }
        )
```

## 5. æœ€å°é™ã®ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–

### 5.1 åŠ¹ç‡çš„ãªãƒãƒƒãƒå–å¾—ï¼ˆSQLiteä¸è¦ï¼‰

```python
# batch_processor.py
from typing import List, Dict, Any
import asyncio
from concurrent.futures import ThreadPoolExecutor

class SimpleBatchProcessor:
    """ã‚·ãƒ³ãƒ—ãƒ«ã§åŠ¹ç‡çš„ãªãƒãƒƒãƒå‡¦ç†"""
    
    def __init__(self, cache: SimpleTwoLayerCache, max_workers: int = 3):
        self.cache = cache
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    async def batch_get_data(self, requests: List[Dict]) -> List[Any]:
        """è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã®ä¸¦åˆ—å–å¾—"""
        results = []
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ä¸€æ‹¬ãƒã‚§ãƒƒã‚¯
        for req in requests:
            cached = await self.cache.get(**req)
            if cached:
                results.append(cached)
            else:
                # JVLinkã‹ã‚‰éåŒæœŸå–å¾—
                data = await self._fetch_async(req)
                results.append(data)
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                await self.cache.set(data, **req)
        
        return results
    
    async def _fetch_async(self, request: Dict) -> Any:
        """éåŒæœŸã§JVLinkã‹ã‚‰å–å¾—"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor,
            self._fetch_from_jvlink,
            request
        )
    
    def _fetch_from_jvlink(self, request: Dict):
        """JVLinkã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆåŒæœŸå‡¦ç†ï¼‰"""
        import pythoncom
        import win32com.client
        
        pythoncom.CoInitialize()
        try:
            jvlink = win32com.client.Dispatch("JVDTLab.JVLink")
            # å®Ÿéš›ã®å–å¾—å‡¦ç†
            return fetch_data(jvlink, **request)
        finally:
            pythoncom.CoUninitialize()

# ä½¿ç”¨ä¾‹
batch_processor = SimpleBatchProcessor(cache)
results = await batch_processor.batch_get_data([
    {"dataspec": "RACE", "from_time": "20250829000000"},
    {"dataspec": "0B12", "from_time": "20250829000000"},
    {"dataspec": "HOSE", "from_time": "20250829000000"}
])
```

## 6. å®Ÿè£…å„ªå…ˆåº¦ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„åŠ¹æœï¼ˆæ”¹è¨‚ç‰ˆï¼‰

### 6.1 ã‚·ãƒ³ãƒ—ãƒ«åŒ–ã«ã‚ˆã‚‹æ”¹å–„åŠ¹æœ

| æœ€é©åŒ–æ‰‹æ³• | å®Ÿè£…é›£æ˜“åº¦ | æ”¹å–„åŠ¹æœ | å„ªå…ˆåº¦ |
|-----------|----------|---------|--------|
| L1ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ | **æœ€ä½** | **80-95%å‰Šæ¸›** | **æœ€é«˜** |
| Redisï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ | ä½ | 60-80%å‰Šæ¸› | é«˜ |
| æ¥ç¶šãƒ—ãƒ¼ãƒ« | ä¸­ | 30-40%å‰Šæ¸› | é«˜ |
| ãƒ¬ã‚¹ãƒãƒ³ã‚¹åœ§ç¸® | ä½ | è»¢é€é‡60-80%å‰Šæ¸› | é«˜ |
| ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚° | ä¸­ | åˆæœŸãƒ¬ã‚¹ãƒãƒ³ã‚¹90%é«˜é€ŸåŒ– | ä¸­ |
| éåŒæœŸãƒãƒƒãƒå‡¦ç† | ä¸­ | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ200-300%å‘ä¸Š | ä¸­ |
| ~~SQLite~~ | - | - | **å‰Šé™¤** |

### 6.2 ã‚·ãƒ³ãƒ—ãƒ«åŒ–ã•ã‚ŒãŸæ®µéšçš„å®Ÿè£…è¨ˆç”»

#### Phase 1: å³åŠ¹æ€§ã®ã‚ã‚‹å®Ÿè£…ï¼ˆ3-5æ—¥ï¼‰
```python
# æœ€å°é™ã®å®Ÿè£…ã§æœ€å¤§ã®åŠ¹æœ
å®Ÿè£…å†…å®¹:
- TTLCacheã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥
- gzipåœ§ç¸®ãƒ¬ã‚¹ãƒãƒ³ã‚¹
- ã‚·ãƒ³ãƒ—ãƒ«ãªæ¥ç¶šç®¡ç†

æœŸå¾…åŠ¹æœ:
â†’ ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“: 40ms â†’ 10msï¼ˆ75%æ”¹å–„ï¼‰
â†’ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ™‚: 0.01msï¼ˆ99.9%æ”¹å–„ï¼‰
```

#### Phase 2: ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£å¯¾å¿œï¼ˆ1-2é€±é–“ï¼‰
```python
# æœ¬ç•ªç’°å¢ƒå‘ã‘æœ€é©åŒ–
å®Ÿè£…å†…å®¹:
- Redisçµ±åˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°API
- éåŒæœŸãƒãƒƒãƒå‡¦ç†

æœŸå¾…åŠ¹æœ:
â†’ ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“: 10ms â†’ 5msï¼ˆ87%æ”¹å–„ï¼‰
â†’ åŒæ™‚æ¥ç¶šæ•°: 10å€å‘ä¸Š
```

#### Phase 3: ã‚¢ãƒ‰ãƒãƒ³ã‚¹ãƒ‰æ©Ÿèƒ½ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
```python
# è¿½åŠ ã®æœ€é©åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
å®Ÿè£…å†…å®¹:
- è³¢ã„ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒãƒ³ã‚°
- é©å¿œçš„TTLèª¿æ•´
- WebSocketãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é…ä¿¡

æœŸå¾…åŠ¹æœ:
â†’ ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“: 5ms â†’ 3msï¼ˆ92%æ”¹å–„ï¼‰
â†’ ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“æ„Ÿé€Ÿåº¦: å¤§å¹…å‘ä¸Š
```

## 7. ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚³ãƒ¼ãƒ‰

```python
# benchmark.py
import asyncio
import time
from statistics import mean, stdev

async def benchmark_endpoint(url: str, iterations: int = 100):
    """ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    times = []
    
    async with httpx.AsyncClient() as client:
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for _ in range(10):
            await client.get(url)
        
        # è¨ˆæ¸¬
        for _ in range(iterations):
            start = time.perf_counter()
            response = await client.get(url)
            end = time.perf_counter()
            
            if response.status_code == 200:
                times.append((end - start) * 1000)  # ms
    
    return {
        'mean': mean(times),
        'stdev': stdev(times),
        'min': min(times),
        'max': max(times),
        'p50': sorted(times)[len(times)//2],
        'p95': sorted(times)[int(len(times)*0.95)],
        'p99': sorted(times)[int(len(times)*0.99)]
    }

# ä½¿ç”¨ä¾‹
results = await benchmark_endpoint(
    'http://localhost:8000/api/v1/data/RACE?from_time=20250829000000'
)
print(f"å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“: {results['mean']:.2f}ms")
print(f"95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {results['p95']:.2f}ms")
```

## ã¾ã¨ã‚

### ğŸ¯ ã‚·ãƒ³ãƒ—ãƒ«åŒ–ã«ã‚ˆã‚‹å¤§ããªãƒ¡ãƒªãƒƒãƒˆ

1. **é–‹ç™ºé€Ÿåº¦ã®å‘ä¸Š**
   - SQLiteå‰Šé™¤ã«ã‚ˆã‚Šå®Ÿè£…ãŒ50%ç°¡ç´ åŒ–
   - ä¿å®ˆæ€§ãŒå¤§å¹…ã«å‘ä¸Š
   - ãƒã‚°ãƒªã‚¹ã‚¯ã®ä½æ¸›

2. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å‘ä¸Š**
   - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ™‚: **0.01ms**ï¼ˆ99.9%é«˜é€ŸåŒ–ï¼‰
   - é€šå¸¸ã‚¢ã‚¯ã‚»ã‚¹: **5-10ms**ï¼ˆ75-87%é«˜é€ŸåŒ–ï¼‰
   - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: æœ€å°é™ã«æŠ‘åˆ¶

3. **é‹ç”¨ã®ç°¡ç´ åŒ–**
   - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†ä¸è¦
   - ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸è¦
   - ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç®¡ç†ä¸è¦

### ğŸ“Š ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¯”è¼ƒ

| æ§‹æˆ | è¤‡é›‘åº¦ | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ | æ¨å¥¨åº¦ |
|------|--------|--------------|--------|
| ãƒ¡ãƒ¢ãƒªã®ã¿ | æœ€ä½ | æœ€é«˜ï¼ˆ0.01msï¼‰ | é–‹ç™º/å°è¦æ¨¡ |
| **ãƒ¡ãƒ¢ãƒª+Redis** | **ä½** | **é«˜ï¼ˆ0.01-2msï¼‰** | **æœ¬ç•ªç’°å¢ƒ âœ…** |
| ~~+SQLite~~ | é«˜ | ä¸­ï¼ˆ5-10msï¼‰ | éæ¨å¥¨ âŒ |

### ğŸš€ æ¨å¥¨å®Ÿè£…æ‰‹é †

1. **ã¾ãšãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã¿ã§é–‹å§‹**ï¼ˆ3æ—¥ï¼‰
2. **å¿…è¦ã«å¿œã˜ã¦Redisè¿½åŠ **ï¼ˆ2æ—¥ï¼‰
3. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿæ–½**
4. **è¦ä»¶ã«å¿œã˜ã¦è¿½åŠ æœ€é©åŒ–**

**çµè«–**: ã‚·ãƒ³ãƒ—ãƒ«ãª2å±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ¡ãƒ¢ãƒª+Redisï¼‰ã§ã€è¤‡é›‘ãªSQLiteå®Ÿè£…ã‚’è¶…ãˆã‚‹æ€§èƒ½ã‚’å®Ÿç¾ã§ãã¾ã™ã€‚